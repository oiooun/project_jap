 # JAP (Jop Assist Program) 
 
 ### 9조 
 2021111713 김도윤     
 2021111716 김송연        
 2021111747 최주현      

----------------------------------------------------------


## 프로젝트 개요     

20대의 취업난이 사회적으로 대두되고 있는 요즘,             
20대 취업난의 문제가 사회생활의 시작인 대학교에서부터 출발한다고 생각하여       
대학에서의 전공만족도와 취업의 상관관계에 궁금증을 갖게 되어 **‘대학생의 전공만족도와 취업과의 상관관계’** 라는 주제를 선정했습니다.   

이와 관련한 연구 논문을 분석해본 결과,        
‘전공만족도가 높을수록 취업에 유리할까?’ 라는 연구 문제와 4개의 연구 가설을 설정하게 되었습니다.        

  1. 전공만족도가 높을수록 전공학점이 높을 것이다.  
  2. 전공학점이 높을수록 취업자신감이 높을 것이다.
  3. 전공만족도가 높을수록 취업에 유리할 것이다.
  4. 대학생활만족도가 취업에 영향을 미치지 않을 것이다.


현재까지 **데이터수집 및 분석한 결과**를 말씀드리겠습니다.     
데이터 수집에서는 1차 데이터인 설문데이터, 2차 데이터로는 산업별_전공계열별_취업자 데이터와 뉴스 기사를 크롤링하여 수집했습니다.      

1차 데이터로는 ‘전공만족도가 높을수록 취업에 유리할까?’의 4가지 가설을 검증했으며       
2차 데이터를 분석한 결과 전공계열별로 취업에 유리한 산업 직군을 파악할 수 있었고,       
알고리즘 개발에 필요한 취업 증진 솔루션들을 얻을 수 있었습니다.            

이 후 **프로젝트 방향**에 대해 말씀드리겠습니다.        
프로젝트의 최종 목표는 잡 어시스트 프로그램을 개발하여 어플로 출시하는 것이고       
서울여대 학우들의 취업 데이터를 수집하여 분석해보고 필요한 역량을 추천해주는 시스템을 구축하고자 합니다.      
나아가 취업에 어려움을 겪는 모든 사람들이 잡 어시스트 프로그램을 통해 도움을 받기를 기대합니다.      



-----------------------


## 코드실행 방법

**구조** : collect (데이터 수집), preprocessing(시각화를 위한 데이터 틀 만들기), viz(데이터 시각화) branch를 만들고 각각의 branch의 function파일을 만들어 코드를 구성했습니다.

Collect.py에서는 크롤링을 통해 검색한 내용에 맞는 정보, 수집한 시간, url를 사용자가 원하는 개수만큼 수집했습니다.

Collect.py에서는 크롤링을 통해 네이버 뉴스에서 사용자가 지정한 키워드에 해당하는 뉴스를 사용자가 원하는 개수만큼, 제목과 URL을 엑셀 파일로 output 해줍니다. 엑셀 파일의 제목에는 크롤링을 실행한 시간에 대한 정보 또한 표시됩니다. 
크롤링을 실행하는 소스는 네이버 뉴스이며 사용자의 input은 원하는 검색어, 원하는 기사의 개수이며 Python의 Requests와 BeautifulSoup 툴을 이용합니다.

코드의 첫 부분에서 필요한 패키지들을 import 해줍니다. 사용하는 툴은 Requests와 BeautifulSoup으로 Requests에는 request를 보내 웹 페이지 소스를 받고 BeautifulSoup에는 웹 페이지 소스를 parsing하여 원하는 정보를 찾습니다.
output으로 엑셀을 저장할 때 크롤링한 현재 날짜와 시간을 파일명에 넣기 위해 date변수를 활용합니다.
input에서 검색할 키워드, 추출할 뉴스 기사를 저장하는 변수인데, query에서 ‘ ‘를 ‘+’로 바꿔주어 URL조건 절에 ‘+’가 요청인자로 들어가지 않게 합니다.
다음 키워드(query)를 URL의 조건절 중 키워드에 해당하는 변수에 대응시켜 요청 URL를 만듭니다. 그리고 request 패키지의 get함수를 이용하여 html코드를 받아옵니다.
받은 코드를 bs4의 BeautifulSoup 함수를 이용하여 parsing합니다. 
다음 원하는 정보를 담을 변수를 생성하는데, 뉴스 기사 정보를 저장할 Dictionary를 생성합니다. Key는 번호로, value는 뉴스 기사 정보로 저장합니다.
idx는 현재 뉴스의 번호이고 cur_page는 네이버 뉴스의 웹페이지인데, 추출려는 기사의 수가 현재 페이지에 있는 기사보다 많은 경우 다음 페이지로 넘어가야 하기 때문에 현 페이지 번호를 기억하도록 변수로 설정합니다. 
Parsing 한 html 코드에서 원하는 정보를 탐색합니다. idx가 news_num보다 작은 동안 아래 코드를 실행하여 원하는 뉴스 기사의 정보를 크롤링합니다. 
크롤링한 뉴스 정보가 담긴 Dictionary를 데이터 프레임으로 변환합니다. 그리고 크롤링한 키워드와 크롤링 날짜를 엑셀 파일명으로 하여 저장합니다. 마지막으로 저장을 완료한 폴더를 띄웁니다. 
결과물은 각 뉴스 기사당 번호, 그리고 뉴스 기사의 제목과 해당 뉴스 기사의 URL이 각각 있는 엑셀파일로 나옵니다. 
 
employment_status.py에서는 앞선 collect.py로 수집한 데이터를 불러옵니다.
데이터 프레임을 전공계열별로 분리하여 설정해줍니다. 
각 데이터 프레임에서 object타입을 float타입으로 변경한 후, 백분율을 구하기 위한 취업자 수의 합을 계산하고 각 산업별 취업자 수를 전체 취업자 수로 나누어 백분율을 구하고 백분율 컬럼값을 기존의 데이터값에 추가하고 print 해줍니다. 






-------------------


## 발전시킨 내용

캡스톤1에서는 설문 데이터를 이용하여 가설을 검증할 때 SPSS를 활용했다면, 이번에는 파이썬 강의에서 배운 라이브러리들을 활용하여 pair plot, heatmap 등 다양한 그래프로 시각화를 진행했습니다.      
우선, 첫 번째로 설문 데이터를 각 항목별(univ_type, major_satisfcation, major_grade, job_confidence, univ_life_satisfaction, employment_status)로 **pair plot** 형식으로 나타내었고, 두 번째로는 데이터 column 간 연관도를 계산하는 함수를 이용하여 **heatmap**으로 표현했습니다. 

또한, 통계 데이터의 경우, 기존에는 엑셀의 툴을 사용하여 C.V 계수 셀을 삭제하는 작업만 했었던 반면, 이번에는 python의 pandas를 활용하여 각 산업별 직업군에 종사하는 사람들의 수를 **비율로 환산**하는 작업을 하였습니다. 이를 통해 각 산업별 취업 현황을 보다 명시적으로 나타낼 수 있게 되었습니다.


---------------

## 기타 특이사항
설문 데이터의 경우 기존에는 spss 프로그램을 사용하여 가설 검증을 완료하였지만, pair plot의 경우, 
